# Importa todas as ferramentas necess√°rias
import pandas as pd
import google.generativeai as genai
import chromadb
import streamlit as st # Importa o Streamlit
import time
import os

# --- PARTE 1: CONFIGURA√á√ÉO E T√çTULO DA P√ÅGINA ---

# Configura o t√≠tulo da p√°gina, √≠cone e layout
st.set_page_config(
    page_title="Assistente de Vendas IA",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Assistente de Vendas com IA")
st.caption("Fa√ßa perguntas sobre os produtos da nossa base de dados.")

# --- PARTE 2: CARREGAMENTO DOS DADOS E SEGURAN√áA DA CHAVE (IMPORTANTE!) ---

# Pede para o usu√°rio inserir a chave de API de forma segura
# Isso evita deixar a chave exposta no c√≥digo
try:
    # Tenta obter a chave dos segredos do Streamlit (para quando estiver online)
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (KeyError, FileNotFoundError):
    # Se n√£o encontrar, pede para o usu√°rio no app
    GOOGLE_API_KEY = st.text_input("Digite sua Chave de API do Google AI Studio:", type="password")

if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    st.warning("Por favor, insira sua chave de API para continuar.")
    st.stop() # Interrompe a execu√ß√£o se a chave n√£o for fornecida

# --- PARTE 3: PROCESSAMENTO DA BASE DE DADOS (S√ì EXECUTA UMA VEZ) ---

# Usamos @st.cache_resource para garantir que esta parte rode apenas uma vez,
# mesmo que o usu√°rio interaja com a p√°gina. Isso economiza tempo e processamento.
@st.cache_resource
def carregar_dados():
    EXCEL_FILE_PATH = 'minha_base.xlsx'
    COLLECTION_NAME = "produtos_collection"
    
    if not os.path.exists(EXCEL_FILE_PATH):
        st.error(f"ERRO: O arquivo '{EXCEL_FILE_PATH}' n√£o foi encontrado.")
        st.stop()
        
    df = pd.read_excel(EXCEL_FILE_PATH)
    documentos = []
    for index, row in df.iterrows():
        texto_linha = f"Refer√™ncia {index + 1}: "
        for coluna, valor in row.items():
            texto_linha += f"{coluna} √© {valor}; "
        documentos.append(texto_linha)

    client = chromadb.Client() # Cliente em mem√≥ria para simplificar
    
    # Deleta a cole√ß√£o se ela j√° existir para garantir dados atualizados
    if len(client.list_collections()) > 0 and COLLECTION_NAME in [c.name for c in client.list_collections()]:
        client.delete_collection(name=COLLECTION_NAME)

    collection = client.create_collection(name=COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    
    model_embedding = 'models/text-embedding-004'
    response = genai.embed_content(model=model_embedding, content=documentos)
    
    collection.add(
        ids=[str(i) for i in range(len(documentos))],
        embeddings=response['embedding'],
        documents=documentos
    )
    
    # Esta mensagem agora aparecer√° na interface web, n√£o no terminal
    st.success("Base de dados carregada e processada com sucesso!")
    return collection

# Carrega a cole√ß√£o (o "c√©rebro" do bot)
collection = carregar_dados()

# --- PARTE 4: L√ìGICA DO CHATBOT COM MEM√ìRIA ---

# Inicializa o hist√≥rico da conversa na sess√£o
if "messages" not in st.session_state:
    st.session_state.messages = []

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a pergunta do usu√°rio
if prompt := st.chat_input("Qual sua d√∫vida?"):
    # Adiciona a pergunta do usu√°rio ao hist√≥rico
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Monta o prompt para o Gemini com o hist√≥rico da conversa
    model_generative = genai.GenerativeModel('gemini-1.5-flash')
    
    # Busca no ChromaDB por contexto relevante
    embedding_prompt = genai.embed_content(model='models/text-embedding-004', content=prompt)['embedding']
    resultados = collection.query(query_embeddings=[embedding_prompt], n_results=5)
    contexto = "\n".join(resultados['documents'][0])
    
    # Constr√≥i um hist√≥rico formatado para o modelo
    historico_formatado = "\n".join([f'{m["role"]}: {m["content"]}' for m in st.session_state.messages])
    
    prompt_final = f"""
    Voc√™ √© um assistente de vendas e deve responder com base no CONTEXTO e no HIST√ìRICO da conversa.

    CONTEXTO:
    {contexto}

    HIST√ìRICO DA CONVERSA:
    {historico_formatado}

    PERGUNTA ATUAL do usu√°rio:
    {prompt}
    
    Com base em tudo isso, forne√ßa uma resposta √∫til e concisa:
    """
    
    # Gera e exibe a resposta do bot
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            response = model_generative.generate_content(prompt_final)
            resposta = response.text
            st.markdown(resposta)
    
    # Adiciona a resposta do bot ao hist√≥rico
    st.session_state.messages.append({"role": "assistant", "content": resposta})