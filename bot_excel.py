# --- Corre√ß√£o para o ChromaDB no Streamlit Cloud ---
__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
# --- Fim da Corre√ß√£o ---

# O resto do seu c√≥digo come√ßa aqui...
import pandas as pd
import google.generativeai as genai
import chromadb
import streamlit as st
import time
import os

# --- PARTE 1: CONFIGURA√á√ÉO E T√çTULO DA P√ÅGINA ---

# Configura o t√≠tulo da p√°gina, √≠cone e layout
st.set_page_config(
    page_title="Assistente de Vendas IA",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Assistente de Vendas com IA")
st.caption("Fa√ßa perguntas sobre os produtos da nossa base de dados.")

# --- PARTE 2: CARREGAMENTO DOS DADOS E SEGURAN√áA DA CHAVE (IMPORTANTE!) ---

# Pede para o usu√°rio inserir a chave de API de forma segura
try:
    # Tenta obter a chave dos segredos do Streamlit (para quando estiver online)
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except (KeyError, FileNotFoundError):
    st.error("ERRO: Chave de API n√£o encontrada nos 'Secrets' do Streamlit. Por favor, configure-a no painel de deploy.")
    st.stop()

if GOOGLE_API_KEY:
    genai.configure(api_key=GOOGLE_API_KEY)
else:
    st.warning("Chave de API configurada, mas vazia.")
    st.stop()

# --- PARTE 3: PROCESSAMENTO DA BASE DE DADOS (S√ì EXECUTA UMA VEZ) ---

@st.cache_resource
def carregar_dados():
    EXCEL_FILE_PATH = 'minha_base.xlsx'
    COLLECTION_NAME = "produtos_collection"
    
    if not os.path.exists(EXCEL_FILE_PATH):
        st.error(f"ERRO: O arquivo de dados '{EXCEL_FILE_PATH}' n√£o foi encontrado no reposit√≥rio.")
        st.stop()
        
    df = pd.read_excel(EXCEL_FILE_PATH)
    documentos = []
    for index, row in df.iterrows():
        texto_linha = f"Refer√™ncia {index + 1}: "
        for coluna, valor in row.items():
            texto_linha += f"{coluna} √© {valor}; "
        documentos.append(texto_linha)

    client = chromadb.Client()
    
    if len(client.list_collections()) > 0 and COLLECTION_NAME in [c.name for c in client.list_collections()]:
        client.delete_collection(name=COLLECTION_NAME)

    collection = client.create_collection(name=COLLECTION_NAME, metadata={"hnsw:space": "cosine"})
    
    model_embedding = 'models/text-embedding-004'
    response = genai.embed_content(model=model_embedding, content=documentos)
    
    collection.add(
        ids=[str(i) for i in range(len(documentos))],
        embeddings=response['embedding'],
        documents=documentos
    )
    
    return collection

# Carrega a cole√ß√£o (o "c√©rebro" do bot)
with st.spinner("Analisando a base de dados... Por favor, aguarde."):
    collection = carregar_dados()

# --- PARTE 4: L√ìGICA DO CHATBOT COM MEM√ìRIA ---

# Inicializa o hist√≥rico da conversa na sess√£o
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Como posso ajudar com os nossos produtos hoje?"}]

# Exibe as mensagens do hist√≥rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Captura a pergunta do usu√°rio
if prompt := st.chat_input("Qual sua d√∫vida?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            model_generative = genai.GenerativeModel('gemini-1.5-flash')
            
            embedding_prompt = genai.embed_content(model='models/text-embedding-004', content=prompt)['embedding']
            resultados = collection.query(query_embeddings=[embedding_prompt], n_results=5)
            contexto = "\n".join(resultados['documents'][0])
            
            historico_formatado = "\n".join([f'{m["role"]}: {m["content"]}' for m in st.session_state.messages])
            
            prompt_final = f"""
            Voc√™ √© um assistente de vendas e deve responder com base no CONTEXTO e no HIST√ìRICO da conversa.

            CONTEXTO:
            {contexto}

            HIST√ìRICO DA CONVERSA:
            {historico_formatado}

            PERGUNTA ATUAL do usu√°rio:
            {prompt}
            
            Com base em tudo isso, forne√ßa uma resposta √∫til e concisa:
            """
            
            response = model_generative.generate_content(prompt_final)
            resposta = response.text
            st.markdown(resposta)
    
    st.session_state.messages.append({"role": "assistant", "content": resposta})
